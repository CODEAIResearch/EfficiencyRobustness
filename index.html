<!DOCTYPE html>
<html>
<head>
      <style>
        body {
            font-family: Arial, sans-serif;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }
        table, th, td {
            border: 1px solid black;
        }
        th, td {
            padding: 8px;
            text-align: center;
        }
        th {
            background-color: #f2f2f2;
        }
        thead th {
            border-bottom: 2px solid black;
        }
        tbody tr:not(:last-child) {
            border-bottom: 1px solid black;
        }
        caption {
            caption-side: top;
            font-weight: bold;
            margin-bottom: 10px;
        }
        .container {
            display: flex;
            justify-content: space-between;
        }
        .figure {
            background-color: #fff;
            padding: 20px;
            margin: 10px;
            border: 1px solid #ccc;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            width: 45%;
        }
        .figure img {
            max-width: 75%;
            height: auto;
        }
        .caption {
            text-align: center;
            margin-top: 10px;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            color: #333;
            background-color: #f9f9f9;
            padding: 5px;
            border-radius: 5px;
        }
        .section-caption {
            text-align: center;
            margin: 20px 0;
            font-weight: bold;
        }
    </style>
    <title>SoK</title>
</head>
<body>
    <h1>Beyond Confidence: Rethinking Uncertainty Calibration in Deep Code Models</h1>
</body>


    <section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p> Deep learning models have shown significant promise in code related tasks. While confidence calibration techniques have been proposed to improve model reliability, their effectiveness in real world tasks with strict accuracy requirements remains uncertain. In this paper, we investigate two key questions: first, whether existing measurement metrics, including confidence scores and uncertainty metrics, are effective for guiding selective prediction; and second, whether existing calibration techniques can improve these metrics to better identify cases where model predictions are likely to succeed or fail.<br>
                
              Through a systematic study using code models for defect prediction and vulnerability detection, our findings reveal that traditional calibration metrics often fail to prioritize high-confidence predictions, which are crucial for practical deployment. To address this, we propose a novel metric based on variation consistency across model predictions, leveraging insights from ensemble methods while reducing computational overhead. Additionally, we introduce a weighted calibration technique that emphasizes high-confidence predictions by assigning greater weight to confident outputs while reducing the influence of low-confidence samples. <br>
              Our empirical results show that the proposed methods improve the ability to distinguish between correct and incorrect predictions upto 96.3%, enhancing selective prediction performance. These insights provide a promising direction for increasing the reliability and practical utility of deep learning-based code models. <br>
               </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->



</html>
